package lapis;


message SystemConfProto {
  //rank for coordinator
  required int32 coordinator = 1;
  //start rank for worker, both start and end ranks are included.
  required int32 worker_start = 2;
  //end rank for worker
  required int32 worker_end = 3;

  //if the following fields are omitted, then use the setting of workers

  //start rank for distributed memory table
  optional int32 memory_start = 4;
  //end rank for distributed memory table
  optional int32 memory_end = 5;
  //start rank for distributed disk table
  optional int32 disk_start = 6;
  //end rank for distributed disk table
  optional int32 disk_end = 7;
}

message DataSourceProto {
  required string name = 1;
  required string id = 2;
  optional string path = 3;
  optional int64 size = 4;

  enum DataType {
    FEATURE = 0;
    LABEL = 1;
  }

  optional DataType type = 5;

  // following three fields will be used for rgb image feature
  optional int32 channels = 6 [default = 1];
  optional int32 height = 7 [default = 0];
  // for one dimension feature, this is the length of the feature
  optional int32 width = 8 [default = 0];

  // offset of record to be read.
  optional int64 offset =9 [default=0];
}

message RGBDatum {
  optional int32 channels = 1 [default = 3];

  // size of the cropped image, default setting is no crop
  optional int32 height = 2 [default = 0];
  optional int32 width = 3 [default = 0];
  required string content = 4;
}

message ParamProto {
  // for the program to identify it, each edge/layer has fixed parameters
  // e.g., "weight" "bias"
  required string name = 1;
  // optional, in most situation, user do not need to config this, the program
  // will calculate it
  repeated int32 shape = 2;

  enum InitMethod {
    kConstant = 0;
    kGaussain = 1;
    kUniform = 2;
    kPretrained = 3;
    // from Toronto Convnet, let a=1/sqrt(fan_in), w*=a after generating from
    // Gaussian distribution
    kGaussainSqrtFanIn = 4;
    // from Toronto Convnet, rectified linear activation, let
    // a=sqrt(3)/sqrt(fan_in), range is [-a, +a]
    kUniformSqrtFanIn = 5;
    // from Theano MLP tutorial, let a=1/sqrt(fan_in+fan_out). for tanh
    // activation, range is [-6a, +6a], for sigmoid activation, range is
    // [-24a, +24a], put the scale factor to value field.
    // <a href="http://deeplearning.net/tutorial/mlp.html"> Theano MLP</a>
    kUniformSqrtFanInOut = 6;
  }
  optional InitMethod init_method = 3 [default = kConstant];
  optional float value = 4 [default = 0];
  optional float low = 5 [default = -1];
  optional float high = 6 [default = 1];
  optional float mean = 7 [default = 0];
  optional float std = 8 [default = 1];
  // optional string partitioner = 4; // e.g., row or column partition
  optional float momentum =9;
  optional float learning_rate =10;
  optional float weight_decay=11;
  repeated float content = 13 [packed = true];
  repeated float history = 14 [packed = true];
}

// both edge and layer only have params
message EdgeProto {
  required string name = 1;
  required string type = 2;
  optional int32 num_output = 3;
  repeated ParamProto param = 4;
  optional bool directed = 5 [default = true];
  optional string top = 6;
  optional string bottom =7;
  // for convolutional and pooling edge.
  optional int32 kernel_size = 8;
  optional int32 stride = 9;
  optional int32 pad = 10;

  // for local response normalization edge
  optional float alpha = 11;
  optional float beta = 12;
  optional int32 local_size = 13;

  // for pooling edge
  enum PoolingMethod {
    kMaxPooling=1;
    kAvgPooling=2;
  }
  optional PoolingMethod pooling_method = 14;
}

message BlobProto{
}

message LayerProto {
  required string name = 1;
  required int32 num_output = 2;
  required string type = 3;
  repeated ParamProto param  = 4;
  repeated string out_edge = 5;
  repeated string in_edge = 6;
  optional string data_source = 7;
}

message NetProto {
  repeated LayerProto layer = 3;
  repeated EdgeProto edge  = 4;
}

// stochastic gradient
message SGDProto {
  required float base_learning_rate = 1;
  optional float base_momentum = 2 [default = 0];
  optional float base_weight_decay = 3 [default = 0];
  optional float final_momentum = 4;
  optional float final_learning_rate = 5;
  optional float final_weight_decay = 6;
  optional int32 learning_rate_change_steps = 7;
  optional int32 momentum_change_steps = 8;
  optional int32 weight_decay_change_steps = 9;

  enum ChangeProto {
    kFixed = 0;
    kInverse_t= 1;
    kExponential = 2;
    kLinear = 3;
  }

  optional ChangeProto learning_rate_change = 10 [default = kInverse_t];
  optional ChangeProto weight_decay_change = 11 [default = kFixed];
  optional ChangeProto momentum_change = 12 [default = kFixed];

  optional int32 total_steps = 13;
  // batchsize is the num of instances processed per step
  optional int32 train_batchsize = 14;
  optional int32 validation_batchsize = 15;
  optional int32 test_batchsize = 16;
}


message PerformanceProto {
  optional float precision = 1;
  optional float recall = 2;
  optional float map = 3;
  optional float precision50 = 4;
}

message TrainerProto {
  // optimizer, may support lbfgs later
  optional SGDProto sgd = 1;

  // start the checkpoint operation after this num steps
  optional int32 checkpoint_after_steps = 2 [default = 0];
  // frequency of checkpoint
  optional int32 checkpoint_every_steps = 3 [default = 0];
  // path prefix for the checkpoint file
  optional string checkpoint_prefix  = 4 [default = "tmp/checkpoint"];
  // the time of checkpoint
  optional int32 checkpoint_step = 5 [default = 0];
  // start display after this num steps
  optional int32 display_after_steps = 6 [default = 0];
  // frequency of display
  optional int32 display_every_steps = 7 [default = 0];
  // path prefix for the display file
  optional string display_prefix = 8 [default = "tmp/display"];

  repeated DataSourceProto train_data = 9;
  repeated DataSourceProto validation_data = 10;
  repeated DataSourceProto test_data = 11;

  //optional PerformanceProto train_perf = 11;
  //optional PerformanceProto validation_perf = 12;
  //optional PerformanceProto test_perf = 13;
  optional string perf_prefix = 14 [default = "tmp/performance"];
}

message ModelConfProto {
  // model name
  optional string name = 1;
  required NetProto net = 2;
  required TrainerProto trainer = 3;
}

message float_vector_message {
  repeated float myfloat = 1;
}
