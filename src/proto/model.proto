package lapis;

message ParamProto {
  // for the program to identify it, each edge/layer has fixed parameters
  // e.g., "weight" "bias"
  required string name = 1;
  // optional, in most situation, user do not need to config this, the program
  // will calculate it
  repeated int32 shape = 2;

  enum InitMethod {
    kConstant = 0;
    // sample gaussian with std and mean
    kGaussain = 1;
    // uniform sampling between low and high
    kUniform = 2;
    // copy the content and history which are from previous training
    kPretrained = 3;
    // from Toronto Convnet, let a=1/sqrt(fan_in), w*=a after generating from
    // Gaussian distribution
    kGaussainSqrtFanIn = 4;
    // from Toronto Convnet, rectified linear activation, let
    // a=sqrt(3)/sqrt(fan_in), range is [-a, +a]; no need to set value=sqrt(3),
    // the program will multiply it.
    kUniformSqrtFanIn = 5;
    // from Theano MLP tutorial, let a=1/sqrt(fan_in+fan_out). for tanh
    // activation, range is [-6a, +6a], for sigmoid activation, range is
    // [-24a, +24a], put the scale factor to value field.
    // <a href="http://deeplearning.net/tutorial/mlp.html"> Theano MLP</a>
    kUniformSqrtFanInOut = 6;
  }
  optional InitMethod init_method = 3 [default = kConstant];

  optional float value = 4 [default = 1.0];
  // for uniform sampling
  optional float low = 5 [default = -1];
  optional float high = 6 [default = 1];
  // for gaussian sampling
  optional float mean = 7 [default = 0];
  optional float std = 8 [default = 1];
  // optional string partitioner = 4; // e.g., row or column partition
  optional float momentum_multiplier =9 [default=1];
  optional float learning_rate_multiplier =10 [default=1];
  optional float weight_decay_multiplier =11 [default=1];
  optional int32 split_threshold=12 [default=5000000];
  optional bool partition=13 [default =false];
  // data of the parameter
  optional DAryProto data = 14;
  // history gradient of the parameter
  optional DAryProto grad = 15;
}

// Only edge have params
message LayerProto {
  required string name = 1;
  required string type = 2;

  optional int32 num_output = 3;
  repeated ParamProto param = 4;
  // for convolutional and pooling edge.
  optional int32 window_size = 8;
  optional int32 stride = 9 [default=1];
  optional int32 pad = 10 [default=0];
  optional int32 num_groups =11 [default=1];

  // for local response normalization edge
  optional float alpha = 12;
  optional float beta = 13;
  optional float knorm = 14 [default =1.0];

  // for pooling edge
  enum PoolingMethod {
    kMaxPooling=1;
    kAvgPooling=2;
  }
  optional PoolingMethod pooling_method = 16;

  // for DataLayer to process image data
  optional int32 cropsize =17;
  optional bool mirror = 18;

  optional float drop_prob = 19 [default=0.5]; // droput probability
  optional DAryProto data = 20;
  optional DAryProto grad = 21;
  optional DAryProto col_data = 22;
  optional DAryProto split_data = 23;
  optional int32 split_size = 24;
  optional int32 split_dim = 25 [default =1];
  optional int32 concat_dim = 26 [default =1];
  optional int32 topk=27 [default=5];
  repeated string top=28;
  repeated string bottom=29;
}

// Edge class proto, which is to simplify the configuation of layer connections.
message EdgeProto{
  // if true, the edge is from layer1 to layer2.
  optional bool directed=1 [default = true];
  // layer name
  optional string layer1=2;
  // layer name
  optional string layer2=3;
}

// used to load image mean provided by caffe, in RGBDirSource
message MeanProto{
  optional int32 num =1 [default =0];
  optional int32 channels = 2 [default =0];
  optional int32 height = 3 [default =0];
  optional int32 width = 4 [default =0];
  repeated float data=5 [packed=true];
  repeated float diff=6 [packed=true];
}

message DAryProto {
  optional int32 partition_dim =1;
  repeated int32 shape = 2;
  repeated float value = 3 [packed=true];
}

message NetProto {
  repeated LayerProto layer = 2;
  repeated EdgeProto edge=3;
}

message PerformanceProto {
  optional float topk_precision = 1 [default=0.0];
  optional float top_precision = 2 [default=0.0];
  optional float loss = 3 [default=0.0];
  optional int32 count =4 [default=0];
  optional int32 step = 5 [default=0];
}

message SolverProto {
  optional int32 step=1 [default=0];
  // start display after this num steps
  optional int32 display_after_steps = 6 [default = 0];
  // frequency of display
  optional int32 display_every_steps = 7 [default = 0];

  // the time of validation
  //optional int32 validation_step = 9 [default = 0];
  // start validation after this num steps
  optional int32 validation_after_steps = 10 [default = 0];
  // frequency of validation
  optional int32 validation_every_steps = 11 [default = 0];

  // the time of test
  //optional int32 test_step = 12 [default = 0];
  // start test after this num steps
  optional int32 test_after_steps = 13 [default = 0];
  // frequency of test
  optional int32 test_every_steps = 14 [default = 0];

  // batchsize is the num of instances processed per step
  optional int32 batchsize = 19;
  // total num of steps for training
  optional int32 train_steps = 20;
  // total num of steps for validation
  optional int32 validation_steps=21;
  // total num of steps for test
  optional int32 test_steps=22;
  // max number of tuples; 3571 is the 500-th prime number
  optional int32 max_splits=23 [default = 3571];
  // SGD parameters
  optional SGDProto sgd=24;
  // There are two basic algorithms for calculating gradients.
  // Different deep learning models use different algorithms.
  enum GradCalcAlg{
    kBackPropagation = 1;
    kContrastiveDivergence = 2;
  }
  optional GradCalcAlg alg= 26 [default = kBackPropagation];

  // partition mode
  enum Partition{
    // only partition data
    kData=1;
    // only partition model
    kModel=2;
    // hybrid partition
    kHybrid=3;
  }
  optional Partition partition=27;
}

message Model{
  // model name
  optional string name = 1;
  optional NetProto net = 2;
  optional SolverProto solver = 3;
}

message Record {
  optional DAryProto image=1;
  optional int32 label=2;
  optional string id=3;
}

message SGDProto {
  optional float learning_rate=1;
  optional float momentum=2;
  optional float weight_decay=3;
  // used in changing learning rate
  optional float gamma = 4;
  optional int32 learning_rate_change_steps = 5;
  enum ChangeProto {
    kFixed = 0;
    kInverse_t= 1;
    kExponential = 2;
    kLinear = 3;
    kStep = 4;
  }
  optional ChangeProto learning_rate_change = 6 [default = kInverse_t];
  optional bool synchronous=7 [default=false];
  // table server handler class identifier
  optional string handler=8 [default="SGDHandler"];
  // start the checkpoint operation after this num steps
  optional int32 checkpoint_after_steps = 9 [default = 0];
  // frequency of checkpoint
  optional int32 checkpoint_every_steps = 10 [default = 0];
  // leave for further extension
  extensions 100 to max;
}

// Tuple key class for Parameter Table
message TKey{
  // Unique identifier of the tuple, i.e, parameter split id
  optional int32 id=1;
  // Tuple version
  optional int32 version=2;
  // MPI rank, e.g. the rank of a worker who sends Get request, used in debug
  optional int32 rank=3;
  // leave for extension
  extensions 100 to max;
}

// Tuple value class for Parameter Table.
message TVal{
  // Identifier string, e.g., type for SGDVal is "SGDVal", "AdaVal" for AdaVal.
  optional string type=1;
  // num of workers whose gradients have been aggregated.
  optional int32 num_aggregate = 4;
  // tuple version the same to that in TKey.
  optional int32 version = 5 ;
  // MPI rank, e.g. the rank of a worker who sends Update request, used in debug
  optional int32 rank=6;
  // parameter id, used in debug and reconstruct the parameter object
  optional int32 param_id=7;
  // split id, used in debug and reconstruct the parameter object
  optional int32 split_id=8;
  // split offset, position (num of floats) of the start of this split to the
  // pos of the first split.
  optional int32 split_offset=9;
  optional float learning_rate_multiplier=10 [default=1.0];
  optional float weight_decay_multiplier=11 [default=1.0];
   // content of the parameter split (i.e., the tuple).
  optional DAryProto data=12;
  // gradient
  optional DAryProto grad = 13;
  // gradient history
  optional DAryProto history=14;
  // extensions used by Child message, e.g., SGDValue
  extensions 100 to max;
}

// Tuple value class used in normal SGD algorithm.
message SGDVal{
  extend TVal {
    optional SGDVal sgd=101;
  }	
}

// Tuple value class used in AdaGrad SGD algorithm.
message AdaVal{
  extend TVal{
    optional AdaVal ada=102;
  }
}
